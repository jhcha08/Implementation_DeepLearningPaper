{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이토치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BN을 정확히 어디에 적용을 해야 할까? Inception을 통과한, 즉 1x1 Conv를 통과한 값에 BN을 적용해야 하는지? 아니면 pre_x + x에 BN을 적용해야 하는지? 아니면 ConvBlock 자체에 BN을 적용시켜서 모두 적용해야 하는지?\n",
    "\n",
    "\n",
    "- Fig.10, 11, 13의 1x1 Conv의 Linear라는게 여기는 BN을 적용하지 않는다는 뜻인듯. 논문에서 \"we used batch-normalization only on top of the traditional layers, but not on top of the summations.\" 라고 나온 부분. traditional layer가 뭔가? 그냥 Conv 레이어를 의미하는건가?\n",
    "\n",
    "\n",
    "- 깃허브 스타를 제일 많이 받은 코드를 따르면, ConvBlock 자체에 BN을 적용시키지만, 각 모듈의 Inception 부분을 통과하고 1x1 Conv 레이어를 통과시킬 때는 BN 적용 안한다. 그렇다면 traditional layer는 그냥 Conv 레이어를 의미하는듯.\n",
    "\n",
    "\n",
    "- 모든 ResNet 관련 모듈에서, 인셉션 모듈을 통과한 값인 residual, 즉 1x1 Conv까지 통과한 값을 0.1~0.3의 값을 곱해서 scaling하고 원래 인풋 값과 더해서 relu를 통과시켜준다. (Fig.20과 \"3.3: Scaling of the Residuals\"을 참고) \n",
    "\n",
    "\n",
    "- 스케일링 값은 깃허브를 참고해서 A, B, C 모듈 각각 0.17, 0.1, 0.2 사용.\n",
    "\n",
    "\n",
    "- BN 관련 참고 깃허브: https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/inceptionresnetv2.py\n",
    "\n",
    "\n",
    "- Reduction B는 두가지 있음. Fig.12는 smaller, Fig.18은 wider 버전에 쓰이는건데, Fig.15의 설명을 보녀 smaller가 v1, wider가 v2에 쓰이는 것이다. 따라서 여기선 smaller, 즉 Fig.12를 쓴다. Fig.18은 v2의 Reduction B에 쓰인다.\n",
    "\n",
    "\n",
    "- 논문에선 Inception-ResNet-V1과 Inception v3의 파라미터 수가 비슷하다는데, 내가 구현한 것으로는 Inception-ResNet-V1이 약 1천만개 정도 더 적다. 이는 Inception v3를 구현할 때 제대로 지정된 채널 수가 많지 않아서 그런걸로 보인다(ex. 해당 논문의 Fig.6 모듈 구현에서 f6 변수 등). 그걸 잘 조절하면 비슷해질 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, stride, padding):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "class Stem(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Stem, self).__init__()\n",
    "        \n",
    "        self.layer = nn.Sequential(\n",
    "            ConvBlock(3, 32, kernel_size=3, stride=2, padding=0),\n",
    "            ConvBlock(32, 32, kernel_size=3, stride=1, padding=0),\n",
    "            ConvBlock(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0),\n",
    "            ConvBlock(64, 80, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(80, 192, kernel_size=3, stride=1, padding=0),\n",
    "            ConvBlock(192, 256, kernel_size=3, stride=2, padding=0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.layer(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class InceptionResA(nn.Module):\n",
    "    def __init__(self, in_ch, scale):\n",
    "        super(InceptionResA, self).__init__()\n",
    "        \n",
    "        self.scaling = scale\n",
    "        \n",
    "        self.branch1 = nn.Sequential(\n",
    "            ConvBlock(in_ch, 32, kernel_size=1, stride=1, padding=0))\n",
    "        \n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBlock(in_ch, 32, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(32, 32, kernel_size=3, stride=1, padding=1))\n",
    "        \n",
    "        self.branch3 = nn.Sequential(\n",
    "            ConvBlock(in_ch, 32, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBlock(32, 32, kernel_size=3, stride=1, padding=1))\n",
    "        \n",
    "        self.conv1x1 = nn.Conv2d(96, 256, kernel_size=1, stride=1, padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        pre_x = x\n",
    "        \n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        \n",
    "        x = torch.cat([x1, x2, x3], dim=1)\n",
    "        \n",
    "        x = self.conv1x1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        out = pre_x + x*self.scaling\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "class ReductionA(nn.Module): # k=192, l=192, m=256, n=384\n",
    "    def __init__(self, in_ch, k, l, m, n):\n",
    "        super(ReductionA, self).__init__()\n",
    "        \n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0))\n",
    "        \n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBlock(in_ch, n, kernel_size=3, stride=2, padding=0))\n",
    "        \n",
    "        self.branch3 = nn.Sequential(\n",
    "            ConvBlock(in_ch, k, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(k, l, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBlock(l, m, kernel_size=3, stride=2, padding=0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        \n",
    "        return torch.cat([x1, x2, x3], dim=1)\n",
    "\n",
    "    \n",
    "class InceptionResB(nn.Module):\n",
    "    def __init__(self, in_ch, scale):\n",
    "        super(InceptionResB, self).__init__()\n",
    "        \n",
    "        self.scaling = scale\n",
    "        \n",
    "        self.branch1 = nn.Sequential(\n",
    "            ConvBlock(in_ch, 128, kernel_size=1, stride=1, padding=0))\n",
    "        \n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBlock(in_ch, 128, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(128, 128, kernel_size=(1,7), stride=1, padding=(0,3)),\n",
    "            ConvBlock(128, 128, kernel_size=(7,1), stride=1, padding=(3,0)))\n",
    "        \n",
    "        self.conv1x1 = nn.Conv2d(256, 896, kernel_size=1, stride=1, padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        pre_x = x\n",
    "        \n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        \n",
    "        x = self.conv1x1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        out = pre_x + x*self.scaling\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class ReductionB(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super(ReductionB, self).__init__()\n",
    "        \n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0))\n",
    "        \n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBlock(in_ch, 256, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(256, 384, kernel_size=3, stride=2, padding=0))\n",
    "        \n",
    "        self.branch3 = nn.Sequential(\n",
    "            ConvBlock(in_ch, 256, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(256, 256, kernel_size=3, stride=2, padding=0))\n",
    "        \n",
    "        self.branch4 = nn.Sequential(\n",
    "            ConvBlock(in_ch, 256, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBlock(256, 256, kernel_size=3, stride=2, padding=0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        x4 = self.branch4(x)\n",
    "        \n",
    "        x = torch.cat([x1, x2, x3, x4], dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class InceptionResC(nn.Module):\n",
    "    def __init__(self, in_ch, scale):\n",
    "        super(InceptionResC, self).__init__()\n",
    "        \n",
    "        self.scaling = scale\n",
    "        \n",
    "        self.branch1 = nn.Sequential(\n",
    "            ConvBlock(in_ch, 192, kernel_size=1, stride=1, padding=0))\n",
    "        \n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBlock(in_ch, 192, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(192, 192, kernel_size=(1,3), stride=1, padding=(0,1)),\n",
    "            ConvBlock(192, 192, kernel_size=(3,1), stride=1, padding=(1,0)))\n",
    "        \n",
    "        self.conv1x1 = nn.Conv2d(384, 1792, kernel_size=1, stride=1, padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        pre_x = x\n",
    "        \n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        \n",
    "        x = self.conv1x1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        out = pre_x + x*self.scaling\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_ResNet_V1(nn.Module):\n",
    "    def __init__(self, num_classes = 1000):\n",
    "        super(Inception_ResNet_V1, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(Stem())\n",
    "        \n",
    "        for _ in range(5):\n",
    "            layers.append(InceptionResA(256, 0.17))\n",
    "            \n",
    "        layers.append(ReductionA(256, 192, 192, 256, 384))\n",
    "        \n",
    "        for _ in range(10):\n",
    "            layers.append(InceptionResB(896, 0.1))\n",
    "            \n",
    "        layers.append(ReductionB(896))\n",
    "        \n",
    "        for _ in range(5):\n",
    "            layers.append(InceptionResC(1792, 0.2))\n",
    "        \n",
    "        self.feature = nn.Sequential(*layers)\n",
    "        \n",
    "        self.globalavgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        self.linear = nn.Linear(1792, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.feature(x)            \n",
    "        x = self.globalavgpool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.linear(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 149, 149]             864\n",
      "       BatchNorm2d-2         [-1, 32, 149, 149]              64\n",
      "              ReLU-3         [-1, 32, 149, 149]               0\n",
      "         ConvBlock-4         [-1, 32, 149, 149]               0\n",
      "            Conv2d-5         [-1, 32, 147, 147]           9,216\n",
      "       BatchNorm2d-6         [-1, 32, 147, 147]              64\n",
      "              ReLU-7         [-1, 32, 147, 147]               0\n",
      "         ConvBlock-8         [-1, 32, 147, 147]               0\n",
      "            Conv2d-9         [-1, 64, 147, 147]          18,432\n",
      "      BatchNorm2d-10         [-1, 64, 147, 147]             128\n",
      "             ReLU-11         [-1, 64, 147, 147]               0\n",
      "        ConvBlock-12         [-1, 64, 147, 147]               0\n",
      "        MaxPool2d-13           [-1, 64, 73, 73]               0\n",
      "           Conv2d-14           [-1, 80, 73, 73]           5,120\n",
      "      BatchNorm2d-15           [-1, 80, 73, 73]             160\n",
      "             ReLU-16           [-1, 80, 73, 73]               0\n",
      "        ConvBlock-17           [-1, 80, 73, 73]               0\n",
      "           Conv2d-18          [-1, 192, 71, 71]         138,240\n",
      "      BatchNorm2d-19          [-1, 192, 71, 71]             384\n",
      "             ReLU-20          [-1, 192, 71, 71]               0\n",
      "        ConvBlock-21          [-1, 192, 71, 71]               0\n",
      "           Conv2d-22          [-1, 256, 35, 35]         442,368\n",
      "      BatchNorm2d-23          [-1, 256, 35, 35]             512\n",
      "             ReLU-24          [-1, 256, 35, 35]               0\n",
      "        ConvBlock-25          [-1, 256, 35, 35]               0\n",
      "             Stem-26          [-1, 256, 35, 35]               0\n",
      "           Conv2d-27           [-1, 32, 35, 35]           8,192\n",
      "      BatchNorm2d-28           [-1, 32, 35, 35]              64\n",
      "             ReLU-29           [-1, 32, 35, 35]               0\n",
      "        ConvBlock-30           [-1, 32, 35, 35]               0\n",
      "           Conv2d-31           [-1, 32, 35, 35]           8,192\n",
      "      BatchNorm2d-32           [-1, 32, 35, 35]              64\n",
      "             ReLU-33           [-1, 32, 35, 35]               0\n",
      "        ConvBlock-34           [-1, 32, 35, 35]               0\n",
      "           Conv2d-35           [-1, 32, 35, 35]           9,216\n",
      "      BatchNorm2d-36           [-1, 32, 35, 35]              64\n",
      "             ReLU-37           [-1, 32, 35, 35]               0\n",
      "        ConvBlock-38           [-1, 32, 35, 35]               0\n",
      "           Conv2d-39           [-1, 32, 35, 35]           8,192\n",
      "      BatchNorm2d-40           [-1, 32, 35, 35]              64\n",
      "             ReLU-41           [-1, 32, 35, 35]               0\n",
      "        ConvBlock-42           [-1, 32, 35, 35]               0\n",
      "           Conv2d-43           [-1, 32, 35, 35]           9,216\n",
      "      BatchNorm2d-44           [-1, 32, 35, 35]              64\n",
      "             ReLU-45           [-1, 32, 35, 35]               0\n",
      "        ConvBlock-46           [-1, 32, 35, 35]               0\n",
      "           Conv2d-47           [-1, 32, 35, 35]           9,216\n",
      "      BatchNorm2d-48           [-1, 32, 35, 35]              64\n",
      "             ReLU-49           [-1, 32, 35, 35]               0\n",
      "        ConvBlock-50           [-1, 32, 35, 35]               0\n",
      "           Conv2d-51          [-1, 256, 35, 35]          24,832\n",
      "             ReLU-52          [-1, 256, 35, 35]               0\n",
      "             ReLU-53          [-1, 256, 35, 35]               0\n",
      "    InceptionResA-54          [-1, 256, 35, 35]               0\n",
      "           Conv2d-55           [-1, 32, 35, 35]           8,192\n",
      "      BatchNorm2d-56           [-1, 32, 35, 35]              64\n",
      "             ReLU-57           [-1, 32, 35, 35]               0\n",
      "        ConvBlock-58           [-1, 32, 35, 35]               0\n",
      "           Conv2d-59           [-1, 32, 35, 35]           8,192\n",
      "      BatchNorm2d-60           [-1, 32, 35, 35]              64\n",
      "             ReLU-61           [-1, 32, 35, 35]               0\n",
      "        ConvBlock-62           [-1, 32, 35, 35]               0\n",
      "           Conv2d-63           [-1, 32, 35, 35]           9,216\n",
      "      BatchNorm2d-64           [-1, 32, 35, 35]              64\n",
      "             ReLU-65           [-1, 32, 35, 35]               0\n",
      "        ConvBlock-66           [-1, 32, 35, 35]               0\n",
      "           Conv2d-67           [-1, 32, 35, 35]           8,192\n",
      "      BatchNorm2d-68           [-1, 32, 35, 35]              64\n",
      "             ReLU-69           [-1, 32, 35, 35]               0\n",
      "        ConvBlock-70           [-1, 32, 35, 35]               0\n",
      "           Conv2d-71           [-1, 32, 35, 35]           9,216\n",
      "      BatchNorm2d-72           [-1, 32, 35, 35]              64\n",
      "             ReLU-73           [-1, 32, 35, 35]               0\n",
      "        ConvBlock-74           [-1, 32, 35, 35]               0\n",
      "           Conv2d-75           [-1, 32, 35, 35]           9,216\n",
      "      BatchNorm2d-76           [-1, 32, 35, 35]              64\n",
      "             ReLU-77           [-1, 32, 35, 35]               0\n",
      "        ConvBlock-78           [-1, 32, 35, 35]               0\n",
      "           Conv2d-79          [-1, 256, 35, 35]          24,832\n",
      "             ReLU-80          [-1, 256, 35, 35]               0\n",
      "             ReLU-81          [-1, 256, 35, 35]               0\n",
      "    InceptionResA-82          [-1, 256, 35, 35]               0\n",
      "           Conv2d-83           [-1, 32, 35, 35]           8,192\n",
      "      BatchNorm2d-84           [-1, 32, 35, 35]              64\n",
      "             ReLU-85           [-1, 32, 35, 35]               0\n",
      "        ConvBlock-86           [-1, 32, 35, 35]               0\n",
      "           Conv2d-87           [-1, 32, 35, 35]           8,192\n",
      "      BatchNorm2d-88           [-1, 32, 35, 35]              64\n",
      "             ReLU-89           [-1, 32, 35, 35]               0\n",
      "        ConvBlock-90           [-1, 32, 35, 35]               0\n",
      "           Conv2d-91           [-1, 32, 35, 35]           9,216\n",
      "      BatchNorm2d-92           [-1, 32, 35, 35]              64\n",
      "             ReLU-93           [-1, 32, 35, 35]               0\n",
      "        ConvBlock-94           [-1, 32, 35, 35]               0\n",
      "           Conv2d-95           [-1, 32, 35, 35]           8,192\n",
      "      BatchNorm2d-96           [-1, 32, 35, 35]              64\n",
      "             ReLU-97           [-1, 32, 35, 35]               0\n",
      "        ConvBlock-98           [-1, 32, 35, 35]               0\n",
      "           Conv2d-99           [-1, 32, 35, 35]           9,216\n",
      "     BatchNorm2d-100           [-1, 32, 35, 35]              64\n",
      "            ReLU-101           [-1, 32, 35, 35]               0\n",
      "       ConvBlock-102           [-1, 32, 35, 35]               0\n",
      "          Conv2d-103           [-1, 32, 35, 35]           9,216\n",
      "     BatchNorm2d-104           [-1, 32, 35, 35]              64\n",
      "            ReLU-105           [-1, 32, 35, 35]               0\n",
      "       ConvBlock-106           [-1, 32, 35, 35]               0\n",
      "          Conv2d-107          [-1, 256, 35, 35]          24,832\n",
      "            ReLU-108          [-1, 256, 35, 35]               0\n",
      "            ReLU-109          [-1, 256, 35, 35]               0\n",
      "   InceptionResA-110          [-1, 256, 35, 35]               0\n",
      "          Conv2d-111           [-1, 32, 35, 35]           8,192\n",
      "     BatchNorm2d-112           [-1, 32, 35, 35]              64\n",
      "            ReLU-113           [-1, 32, 35, 35]               0\n",
      "       ConvBlock-114           [-1, 32, 35, 35]               0\n",
      "          Conv2d-115           [-1, 32, 35, 35]           8,192\n",
      "     BatchNorm2d-116           [-1, 32, 35, 35]              64\n",
      "            ReLU-117           [-1, 32, 35, 35]               0\n",
      "       ConvBlock-118           [-1, 32, 35, 35]               0\n",
      "          Conv2d-119           [-1, 32, 35, 35]           9,216\n",
      "     BatchNorm2d-120           [-1, 32, 35, 35]              64\n",
      "            ReLU-121           [-1, 32, 35, 35]               0\n",
      "       ConvBlock-122           [-1, 32, 35, 35]               0\n",
      "          Conv2d-123           [-1, 32, 35, 35]           8,192\n",
      "     BatchNorm2d-124           [-1, 32, 35, 35]              64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ReLU-125           [-1, 32, 35, 35]               0\n",
      "       ConvBlock-126           [-1, 32, 35, 35]               0\n",
      "          Conv2d-127           [-1, 32, 35, 35]           9,216\n",
      "     BatchNorm2d-128           [-1, 32, 35, 35]              64\n",
      "            ReLU-129           [-1, 32, 35, 35]               0\n",
      "       ConvBlock-130           [-1, 32, 35, 35]               0\n",
      "          Conv2d-131           [-1, 32, 35, 35]           9,216\n",
      "     BatchNorm2d-132           [-1, 32, 35, 35]              64\n",
      "            ReLU-133           [-1, 32, 35, 35]               0\n",
      "       ConvBlock-134           [-1, 32, 35, 35]               0\n",
      "          Conv2d-135          [-1, 256, 35, 35]          24,832\n",
      "            ReLU-136          [-1, 256, 35, 35]               0\n",
      "            ReLU-137          [-1, 256, 35, 35]               0\n",
      "   InceptionResA-138          [-1, 256, 35, 35]               0\n",
      "          Conv2d-139           [-1, 32, 35, 35]           8,192\n",
      "     BatchNorm2d-140           [-1, 32, 35, 35]              64\n",
      "            ReLU-141           [-1, 32, 35, 35]               0\n",
      "       ConvBlock-142           [-1, 32, 35, 35]               0\n",
      "          Conv2d-143           [-1, 32, 35, 35]           8,192\n",
      "     BatchNorm2d-144           [-1, 32, 35, 35]              64\n",
      "            ReLU-145           [-1, 32, 35, 35]               0\n",
      "       ConvBlock-146           [-1, 32, 35, 35]               0\n",
      "          Conv2d-147           [-1, 32, 35, 35]           9,216\n",
      "     BatchNorm2d-148           [-1, 32, 35, 35]              64\n",
      "            ReLU-149           [-1, 32, 35, 35]               0\n",
      "       ConvBlock-150           [-1, 32, 35, 35]               0\n",
      "          Conv2d-151           [-1, 32, 35, 35]           8,192\n",
      "     BatchNorm2d-152           [-1, 32, 35, 35]              64\n",
      "            ReLU-153           [-1, 32, 35, 35]               0\n",
      "       ConvBlock-154           [-1, 32, 35, 35]               0\n",
      "          Conv2d-155           [-1, 32, 35, 35]           9,216\n",
      "     BatchNorm2d-156           [-1, 32, 35, 35]              64\n",
      "            ReLU-157           [-1, 32, 35, 35]               0\n",
      "       ConvBlock-158           [-1, 32, 35, 35]               0\n",
      "          Conv2d-159           [-1, 32, 35, 35]           9,216\n",
      "     BatchNorm2d-160           [-1, 32, 35, 35]              64\n",
      "            ReLU-161           [-1, 32, 35, 35]               0\n",
      "       ConvBlock-162           [-1, 32, 35, 35]               0\n",
      "          Conv2d-163          [-1, 256, 35, 35]          24,832\n",
      "            ReLU-164          [-1, 256, 35, 35]               0\n",
      "            ReLU-165          [-1, 256, 35, 35]               0\n",
      "   InceptionResA-166          [-1, 256, 35, 35]               0\n",
      "       MaxPool2d-167          [-1, 256, 17, 17]               0\n",
      "          Conv2d-168          [-1, 384, 17, 17]         884,736\n",
      "     BatchNorm2d-169          [-1, 384, 17, 17]             768\n",
      "            ReLU-170          [-1, 384, 17, 17]               0\n",
      "       ConvBlock-171          [-1, 384, 17, 17]               0\n",
      "          Conv2d-172          [-1, 192, 35, 35]          49,152\n",
      "     BatchNorm2d-173          [-1, 192, 35, 35]             384\n",
      "            ReLU-174          [-1, 192, 35, 35]               0\n",
      "       ConvBlock-175          [-1, 192, 35, 35]               0\n",
      "          Conv2d-176          [-1, 192, 35, 35]         331,776\n",
      "     BatchNorm2d-177          [-1, 192, 35, 35]             384\n",
      "            ReLU-178          [-1, 192, 35, 35]               0\n",
      "       ConvBlock-179          [-1, 192, 35, 35]               0\n",
      "          Conv2d-180          [-1, 256, 17, 17]         442,368\n",
      "     BatchNorm2d-181          [-1, 256, 17, 17]             512\n",
      "            ReLU-182          [-1, 256, 17, 17]               0\n",
      "       ConvBlock-183          [-1, 256, 17, 17]               0\n",
      "      ReductionA-184          [-1, 896, 17, 17]               0\n",
      "          Conv2d-185          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-186          [-1, 128, 17, 17]             256\n",
      "            ReLU-187          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-188          [-1, 128, 17, 17]               0\n",
      "          Conv2d-189          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-190          [-1, 128, 17, 17]             256\n",
      "            ReLU-191          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-192          [-1, 128, 17, 17]               0\n",
      "          Conv2d-193          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-194          [-1, 128, 17, 17]             256\n",
      "            ReLU-195          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-196          [-1, 128, 17, 17]               0\n",
      "          Conv2d-197          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-198          [-1, 128, 17, 17]             256\n",
      "            ReLU-199          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-200          [-1, 128, 17, 17]               0\n",
      "          Conv2d-201          [-1, 896, 17, 17]         230,272\n",
      "            ReLU-202          [-1, 896, 17, 17]               0\n",
      "            ReLU-203          [-1, 896, 17, 17]               0\n",
      "   InceptionResB-204          [-1, 896, 17, 17]               0\n",
      "          Conv2d-205          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-206          [-1, 128, 17, 17]             256\n",
      "            ReLU-207          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-208          [-1, 128, 17, 17]               0\n",
      "          Conv2d-209          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-210          [-1, 128, 17, 17]             256\n",
      "            ReLU-211          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-212          [-1, 128, 17, 17]               0\n",
      "          Conv2d-213          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-214          [-1, 128, 17, 17]             256\n",
      "            ReLU-215          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-216          [-1, 128, 17, 17]               0\n",
      "          Conv2d-217          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-218          [-1, 128, 17, 17]             256\n",
      "            ReLU-219          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-220          [-1, 128, 17, 17]               0\n",
      "          Conv2d-221          [-1, 896, 17, 17]         230,272\n",
      "            ReLU-222          [-1, 896, 17, 17]               0\n",
      "            ReLU-223          [-1, 896, 17, 17]               0\n",
      "   InceptionResB-224          [-1, 896, 17, 17]               0\n",
      "          Conv2d-225          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-226          [-1, 128, 17, 17]             256\n",
      "            ReLU-227          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-228          [-1, 128, 17, 17]               0\n",
      "          Conv2d-229          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-230          [-1, 128, 17, 17]             256\n",
      "            ReLU-231          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-232          [-1, 128, 17, 17]               0\n",
      "          Conv2d-233          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-234          [-1, 128, 17, 17]             256\n",
      "            ReLU-235          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-236          [-1, 128, 17, 17]               0\n",
      "          Conv2d-237          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-238          [-1, 128, 17, 17]             256\n",
      "            ReLU-239          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-240          [-1, 128, 17, 17]               0\n",
      "          Conv2d-241          [-1, 896, 17, 17]         230,272\n",
      "            ReLU-242          [-1, 896, 17, 17]               0\n",
      "            ReLU-243          [-1, 896, 17, 17]               0\n",
      "   InceptionResB-244          [-1, 896, 17, 17]               0\n",
      "          Conv2d-245          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-246          [-1, 128, 17, 17]             256\n",
      "            ReLU-247          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-248          [-1, 128, 17, 17]               0\n",
      "          Conv2d-249          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-250          [-1, 128, 17, 17]             256\n",
      "            ReLU-251          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-252          [-1, 128, 17, 17]               0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Conv2d-253          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-254          [-1, 128, 17, 17]             256\n",
      "            ReLU-255          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-256          [-1, 128, 17, 17]               0\n",
      "          Conv2d-257          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-258          [-1, 128, 17, 17]             256\n",
      "            ReLU-259          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-260          [-1, 128, 17, 17]               0\n",
      "          Conv2d-261          [-1, 896, 17, 17]         230,272\n",
      "            ReLU-262          [-1, 896, 17, 17]               0\n",
      "            ReLU-263          [-1, 896, 17, 17]               0\n",
      "   InceptionResB-264          [-1, 896, 17, 17]               0\n",
      "          Conv2d-265          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-266          [-1, 128, 17, 17]             256\n",
      "            ReLU-267          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-268          [-1, 128, 17, 17]               0\n",
      "          Conv2d-269          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-270          [-1, 128, 17, 17]             256\n",
      "            ReLU-271          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-272          [-1, 128, 17, 17]               0\n",
      "          Conv2d-273          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-274          [-1, 128, 17, 17]             256\n",
      "            ReLU-275          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-276          [-1, 128, 17, 17]               0\n",
      "          Conv2d-277          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-278          [-1, 128, 17, 17]             256\n",
      "            ReLU-279          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-280          [-1, 128, 17, 17]               0\n",
      "          Conv2d-281          [-1, 896, 17, 17]         230,272\n",
      "            ReLU-282          [-1, 896, 17, 17]               0\n",
      "            ReLU-283          [-1, 896, 17, 17]               0\n",
      "   InceptionResB-284          [-1, 896, 17, 17]               0\n",
      "          Conv2d-285          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-286          [-1, 128, 17, 17]             256\n",
      "            ReLU-287          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-288          [-1, 128, 17, 17]               0\n",
      "          Conv2d-289          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-290          [-1, 128, 17, 17]             256\n",
      "            ReLU-291          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-292          [-1, 128, 17, 17]               0\n",
      "          Conv2d-293          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-294          [-1, 128, 17, 17]             256\n",
      "            ReLU-295          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-296          [-1, 128, 17, 17]               0\n",
      "          Conv2d-297          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-298          [-1, 128, 17, 17]             256\n",
      "            ReLU-299          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-300          [-1, 128, 17, 17]               0\n",
      "          Conv2d-301          [-1, 896, 17, 17]         230,272\n",
      "            ReLU-302          [-1, 896, 17, 17]               0\n",
      "            ReLU-303          [-1, 896, 17, 17]               0\n",
      "   InceptionResB-304          [-1, 896, 17, 17]               0\n",
      "          Conv2d-305          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-306          [-1, 128, 17, 17]             256\n",
      "            ReLU-307          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-308          [-1, 128, 17, 17]               0\n",
      "          Conv2d-309          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-310          [-1, 128, 17, 17]             256\n",
      "            ReLU-311          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-312          [-1, 128, 17, 17]               0\n",
      "          Conv2d-313          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-314          [-1, 128, 17, 17]             256\n",
      "            ReLU-315          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-316          [-1, 128, 17, 17]               0\n",
      "          Conv2d-317          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-318          [-1, 128, 17, 17]             256\n",
      "            ReLU-319          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-320          [-1, 128, 17, 17]               0\n",
      "          Conv2d-321          [-1, 896, 17, 17]         230,272\n",
      "            ReLU-322          [-1, 896, 17, 17]               0\n",
      "            ReLU-323          [-1, 896, 17, 17]               0\n",
      "   InceptionResB-324          [-1, 896, 17, 17]               0\n",
      "          Conv2d-325          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-326          [-1, 128, 17, 17]             256\n",
      "            ReLU-327          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-328          [-1, 128, 17, 17]               0\n",
      "          Conv2d-329          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-330          [-1, 128, 17, 17]             256\n",
      "            ReLU-331          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-332          [-1, 128, 17, 17]               0\n",
      "          Conv2d-333          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-334          [-1, 128, 17, 17]             256\n",
      "            ReLU-335          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-336          [-1, 128, 17, 17]               0\n",
      "          Conv2d-337          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-338          [-1, 128, 17, 17]             256\n",
      "            ReLU-339          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-340          [-1, 128, 17, 17]               0\n",
      "          Conv2d-341          [-1, 896, 17, 17]         230,272\n",
      "            ReLU-342          [-1, 896, 17, 17]               0\n",
      "            ReLU-343          [-1, 896, 17, 17]               0\n",
      "   InceptionResB-344          [-1, 896, 17, 17]               0\n",
      "          Conv2d-345          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-346          [-1, 128, 17, 17]             256\n",
      "            ReLU-347          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-348          [-1, 128, 17, 17]               0\n",
      "          Conv2d-349          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-350          [-1, 128, 17, 17]             256\n",
      "            ReLU-351          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-352          [-1, 128, 17, 17]               0\n",
      "          Conv2d-353          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-354          [-1, 128, 17, 17]             256\n",
      "            ReLU-355          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-356          [-1, 128, 17, 17]               0\n",
      "          Conv2d-357          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-358          [-1, 128, 17, 17]             256\n",
      "            ReLU-359          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-360          [-1, 128, 17, 17]               0\n",
      "          Conv2d-361          [-1, 896, 17, 17]         230,272\n",
      "            ReLU-362          [-1, 896, 17, 17]               0\n",
      "            ReLU-363          [-1, 896, 17, 17]               0\n",
      "   InceptionResB-364          [-1, 896, 17, 17]               0\n",
      "          Conv2d-365          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-366          [-1, 128, 17, 17]             256\n",
      "            ReLU-367          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-368          [-1, 128, 17, 17]               0\n",
      "          Conv2d-369          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-370          [-1, 128, 17, 17]             256\n",
      "            ReLU-371          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-372          [-1, 128, 17, 17]               0\n",
      "          Conv2d-373          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-374          [-1, 128, 17, 17]             256\n",
      "            ReLU-375          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-376          [-1, 128, 17, 17]               0\n",
      "          Conv2d-377          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-378          [-1, 128, 17, 17]             256\n",
      "            ReLU-379          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-380          [-1, 128, 17, 17]               0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Conv2d-381          [-1, 896, 17, 17]         230,272\n",
      "            ReLU-382          [-1, 896, 17, 17]               0\n",
      "            ReLU-383          [-1, 896, 17, 17]               0\n",
      "   InceptionResB-384          [-1, 896, 17, 17]               0\n",
      "       MaxPool2d-385            [-1, 896, 8, 8]               0\n",
      "          Conv2d-386          [-1, 256, 17, 17]         229,376\n",
      "     BatchNorm2d-387          [-1, 256, 17, 17]             512\n",
      "            ReLU-388          [-1, 256, 17, 17]               0\n",
      "       ConvBlock-389          [-1, 256, 17, 17]               0\n",
      "          Conv2d-390            [-1, 384, 8, 8]         884,736\n",
      "     BatchNorm2d-391            [-1, 384, 8, 8]             768\n",
      "            ReLU-392            [-1, 384, 8, 8]               0\n",
      "       ConvBlock-393            [-1, 384, 8, 8]               0\n",
      "          Conv2d-394          [-1, 256, 17, 17]         229,376\n",
      "     BatchNorm2d-395          [-1, 256, 17, 17]             512\n",
      "            ReLU-396          [-1, 256, 17, 17]               0\n",
      "       ConvBlock-397          [-1, 256, 17, 17]               0\n",
      "          Conv2d-398            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-399            [-1, 256, 8, 8]             512\n",
      "            ReLU-400            [-1, 256, 8, 8]               0\n",
      "       ConvBlock-401            [-1, 256, 8, 8]               0\n",
      "          Conv2d-402          [-1, 256, 17, 17]         229,376\n",
      "     BatchNorm2d-403          [-1, 256, 17, 17]             512\n",
      "            ReLU-404          [-1, 256, 17, 17]               0\n",
      "       ConvBlock-405          [-1, 256, 17, 17]               0\n",
      "          Conv2d-406          [-1, 256, 17, 17]         589,824\n",
      "     BatchNorm2d-407          [-1, 256, 17, 17]             512\n",
      "            ReLU-408          [-1, 256, 17, 17]               0\n",
      "       ConvBlock-409          [-1, 256, 17, 17]               0\n",
      "          Conv2d-410            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-411            [-1, 256, 8, 8]             512\n",
      "            ReLU-412            [-1, 256, 8, 8]               0\n",
      "       ConvBlock-413            [-1, 256, 8, 8]               0\n",
      "      ReductionB-414           [-1, 1792, 8, 8]               0\n",
      "          Conv2d-415            [-1, 192, 8, 8]         344,064\n",
      "     BatchNorm2d-416            [-1, 192, 8, 8]             384\n",
      "            ReLU-417            [-1, 192, 8, 8]               0\n",
      "       ConvBlock-418            [-1, 192, 8, 8]               0\n",
      "          Conv2d-419            [-1, 192, 8, 8]         344,064\n",
      "     BatchNorm2d-420            [-1, 192, 8, 8]             384\n",
      "            ReLU-421            [-1, 192, 8, 8]               0\n",
      "       ConvBlock-422            [-1, 192, 8, 8]               0\n",
      "          Conv2d-423            [-1, 192, 8, 8]         110,592\n",
      "     BatchNorm2d-424            [-1, 192, 8, 8]             384\n",
      "            ReLU-425            [-1, 192, 8, 8]               0\n",
      "       ConvBlock-426            [-1, 192, 8, 8]               0\n",
      "          Conv2d-427            [-1, 192, 8, 8]         110,592\n",
      "     BatchNorm2d-428            [-1, 192, 8, 8]             384\n",
      "            ReLU-429            [-1, 192, 8, 8]               0\n",
      "       ConvBlock-430            [-1, 192, 8, 8]               0\n",
      "          Conv2d-431           [-1, 1792, 8, 8]         689,920\n",
      "            ReLU-432           [-1, 1792, 8, 8]               0\n",
      "            ReLU-433           [-1, 1792, 8, 8]               0\n",
      "   InceptionResC-434           [-1, 1792, 8, 8]               0\n",
      "          Conv2d-435            [-1, 192, 8, 8]         344,064\n",
      "     BatchNorm2d-436            [-1, 192, 8, 8]             384\n",
      "            ReLU-437            [-1, 192, 8, 8]               0\n",
      "       ConvBlock-438            [-1, 192, 8, 8]               0\n",
      "          Conv2d-439            [-1, 192, 8, 8]         344,064\n",
      "     BatchNorm2d-440            [-1, 192, 8, 8]             384\n",
      "            ReLU-441            [-1, 192, 8, 8]               0\n",
      "       ConvBlock-442            [-1, 192, 8, 8]               0\n",
      "          Conv2d-443            [-1, 192, 8, 8]         110,592\n",
      "     BatchNorm2d-444            [-1, 192, 8, 8]             384\n",
      "            ReLU-445            [-1, 192, 8, 8]               0\n",
      "       ConvBlock-446            [-1, 192, 8, 8]               0\n",
      "          Conv2d-447            [-1, 192, 8, 8]         110,592\n",
      "     BatchNorm2d-448            [-1, 192, 8, 8]             384\n",
      "            ReLU-449            [-1, 192, 8, 8]               0\n",
      "       ConvBlock-450            [-1, 192, 8, 8]               0\n",
      "          Conv2d-451           [-1, 1792, 8, 8]         689,920\n",
      "            ReLU-452           [-1, 1792, 8, 8]               0\n",
      "            ReLU-453           [-1, 1792, 8, 8]               0\n",
      "   InceptionResC-454           [-1, 1792, 8, 8]               0\n",
      "          Conv2d-455            [-1, 192, 8, 8]         344,064\n",
      "     BatchNorm2d-456            [-1, 192, 8, 8]             384\n",
      "            ReLU-457            [-1, 192, 8, 8]               0\n",
      "       ConvBlock-458            [-1, 192, 8, 8]               0\n",
      "          Conv2d-459            [-1, 192, 8, 8]         344,064\n",
      "     BatchNorm2d-460            [-1, 192, 8, 8]             384\n",
      "            ReLU-461            [-1, 192, 8, 8]               0\n",
      "       ConvBlock-462            [-1, 192, 8, 8]               0\n",
      "          Conv2d-463            [-1, 192, 8, 8]         110,592\n",
      "     BatchNorm2d-464            [-1, 192, 8, 8]             384\n",
      "            ReLU-465            [-1, 192, 8, 8]               0\n",
      "       ConvBlock-466            [-1, 192, 8, 8]               0\n",
      "          Conv2d-467            [-1, 192, 8, 8]         110,592\n",
      "     BatchNorm2d-468            [-1, 192, 8, 8]             384\n",
      "            ReLU-469            [-1, 192, 8, 8]               0\n",
      "       ConvBlock-470            [-1, 192, 8, 8]               0\n",
      "          Conv2d-471           [-1, 1792, 8, 8]         689,920\n",
      "            ReLU-472           [-1, 1792, 8, 8]               0\n",
      "            ReLU-473           [-1, 1792, 8, 8]               0\n",
      "   InceptionResC-474           [-1, 1792, 8, 8]               0\n",
      "          Conv2d-475            [-1, 192, 8, 8]         344,064\n",
      "     BatchNorm2d-476            [-1, 192, 8, 8]             384\n",
      "            ReLU-477            [-1, 192, 8, 8]               0\n",
      "       ConvBlock-478            [-1, 192, 8, 8]               0\n",
      "          Conv2d-479            [-1, 192, 8, 8]         344,064\n",
      "     BatchNorm2d-480            [-1, 192, 8, 8]             384\n",
      "            ReLU-481            [-1, 192, 8, 8]               0\n",
      "       ConvBlock-482            [-1, 192, 8, 8]               0\n",
      "          Conv2d-483            [-1, 192, 8, 8]         110,592\n",
      "     BatchNorm2d-484            [-1, 192, 8, 8]             384\n",
      "            ReLU-485            [-1, 192, 8, 8]               0\n",
      "       ConvBlock-486            [-1, 192, 8, 8]               0\n",
      "          Conv2d-487            [-1, 192, 8, 8]         110,592\n",
      "     BatchNorm2d-488            [-1, 192, 8, 8]             384\n",
      "            ReLU-489            [-1, 192, 8, 8]               0\n",
      "       ConvBlock-490            [-1, 192, 8, 8]               0\n",
      "          Conv2d-491           [-1, 1792, 8, 8]         689,920\n",
      "            ReLU-492           [-1, 1792, 8, 8]               0\n",
      "            ReLU-493           [-1, 1792, 8, 8]               0\n",
      "   InceptionResC-494           [-1, 1792, 8, 8]               0\n",
      "          Conv2d-495            [-1, 192, 8, 8]         344,064\n",
      "     BatchNorm2d-496            [-1, 192, 8, 8]             384\n",
      "            ReLU-497            [-1, 192, 8, 8]               0\n",
      "       ConvBlock-498            [-1, 192, 8, 8]               0\n",
      "          Conv2d-499            [-1, 192, 8, 8]         344,064\n",
      "     BatchNorm2d-500            [-1, 192, 8, 8]             384\n",
      "            ReLU-501            [-1, 192, 8, 8]               0\n",
      "       ConvBlock-502            [-1, 192, 8, 8]               0\n",
      "          Conv2d-503            [-1, 192, 8, 8]         110,592\n",
      "     BatchNorm2d-504            [-1, 192, 8, 8]             384\n",
      "            ReLU-505            [-1, 192, 8, 8]               0\n",
      "       ConvBlock-506            [-1, 192, 8, 8]               0\n",
      "          Conv2d-507            [-1, 192, 8, 8]         110,592\n",
      "     BatchNorm2d-508            [-1, 192, 8, 8]             384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ReLU-509            [-1, 192, 8, 8]               0\n",
      "       ConvBlock-510            [-1, 192, 8, 8]               0\n",
      "          Conv2d-511           [-1, 1792, 8, 8]         689,920\n",
      "            ReLU-512           [-1, 1792, 8, 8]               0\n",
      "            ReLU-513           [-1, 1792, 8, 8]               0\n",
      "   InceptionResC-514           [-1, 1792, 8, 8]               0\n",
      "AdaptiveAvgPool2d-515           [-1, 1792, 1, 1]               0\n",
      "         Dropout-516           [-1, 1792, 1, 1]               0\n",
      "          Linear-517                 [-1, 1000]       1,793,000\n",
      "================================================================\n",
      "Total params: 22,756,328\n",
      "Trainable params: 22,756,328\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.02\n",
      "Forward/backward pass size (MB): 409.69\n",
      "Params size (MB): 86.81\n",
      "Estimated Total Size (MB): 497.52\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    from torchsummary import summary\n",
    "    model = Inception_ResNet_V1()\n",
    "    summary(model, (3,299,299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22,756,328 total parameters.\n",
      "22,756,328 training parameters.\n"
     ]
    }
   ],
   "source": [
    "# Find total parameters and trainable parameters\n",
    "model = Inception_ResNet_V1()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'{total_trainable_params:,} training parameters.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
